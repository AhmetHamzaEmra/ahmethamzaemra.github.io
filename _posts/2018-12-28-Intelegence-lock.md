---
layout: post
title: "Intelegence lock"
tags: [Deep Learning, Artificial Intelengece]
---


![](https://camo.githubusercontent.com/d764976b343cce451c31a86311ddb94094062f3a/687474703a2f2f63646e2e6e657874676f762e636f6d2f6d656469612f696d672f75706c6f61642f323031372f30342f31342f303431343137637962657270726f74656374696f6e4e472e6a7067)

As Andrej Karpathy said, Software2.0 is creating revelation in the field of software engineering. Now AI is taking most of the heavy lifting in complexity and giving incredible results in the industry. One of this application is locks with face recognition. Although it sounds simple to combine these two technologies, it is more complex than most other applications.

First of all, we have a problem with a dataset. It is very hard to collect more than a couple of pictures of every person that we need to give access. If we can get a couple hundred pictures per person, One can easily train a simple model to do classification. But let us say we have 1000 employees that we want to give access. we need to get ~1.000.000 employee pictures and couple hundred pictures from other people in the World so the model can know if unknown people to lock to the door. On top of this, if we hire anybody else, We would need to train again. Screw that, we need another method. So we need a method that we can just use one picture per person to classify all of them and doesnâ€™t need to train when someone changes. That method is Landmark technique. Letâ€™s mark specific points in the face. Then we can use this transformation to classify people. Since everybody will have a different distance between two eyes or eyes and notice (except the twins!) we can tread this new measurement as embedding and if they are too close to each other in the multidimensional space we can they are the same person, vice versa.


Letâ€™s see how can we implement this face landmarks model. Let's start with finding a dataset. If you google it you will find tons of dataset ([like this one](https://www.kaggle.com/drgilermo/face-images-with-marked-landmark-points)) Then you just train a neural network to map faces to landmark measurements. Let's see a simple implementation with keras. Our goal is to find and mark 15 face landmarks for this dataset. Here are some examples from the dataset.

![](https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/1.png)

```python
# load the dataset and clean a bit 
face_images_db = np.load('face_images.npz')['face_images']
facial_keypoints_df = pd.read_csv('facial_keypoints.csv')

numMissingKeypoints = facial_keypoints_df.isnull().sum(axis=1)
allKeypointsPresentInds = np.nonzero(numMissingKeypoints == 0)[0]

faceImagesDB = face_images_db[:,:,allKeypointsPresentInds]
facialKeypointsDF = facial_keypoints_df.iloc[allKeypointsPresentInds,:].reset_index(drop=True)

(imHeight, imWidth, numImages) = faceImagesDB.shape
numKeypoints = facialKeypointsDF.shape[1] / 2

print('number of remaining images = %d' %(numImages))
print('image dimentions = (%d,%d)' %(imHeight,imWidth))
print('number of facial keypoints = %d' %(numKeypoints))
# number of remaining images = 2140
# image dimentions = (96,96)
# number of facial keypoints = 15

# Some preprocessing 
Y = facialKeypointsDF.as_matrix()
Y_mean = Y.mean()
Y_std = Y.std()
Y = (Y - Y_mean) / Y_std

X = np.transpose(faceImagesDB, (2,0,1))
X/=255
X = X.reshape((-1,96,96,1))
```

Now we are ready for the training. We will create a simple CNN for this task. 

```python
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(96,96,1)))
model.add(Conv2D(32,kernel_size=(3,3)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(30, activation='linear'))
sgd = SGD(lr=0.0001,momentum = 0.9,nesterov=True)
model.compile(loss="mean_squared_error",optimizer=sgd)
model.fit(X,Y,epochs=20,validation_split=0.2,verbose=1)
```

So after the training, we can simply predict the landmarks and use it in the future. Good things about this method are we don't need to train it for the new people. 

![](https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/2.png)

Blue dots are predicted values and red ones are ground truth. As we can see even simple network is quite good at marking them. 

Although it is not the best way to implement this, this method also works just fine. Ideally, We should use triple loss function to create embeddings. The triple loss is an Idea of finding best parameters to separate and recognize people. In nutshell, We want the network to get the same person embeddings closer and if they are different then separate them in multidimensional space. For implementation, we need three photos. Two of the same person and One from someone else. We will just fit the network and after the forward pass, we will check how close same persons face embeddings are and How far with the different person. So with this loss network will be learning to separate people. 


Okay, since we solved the face recognition, we have another problem. What happens if someone holds a picture of me to get in. or How can we make sure they are live people and not someone with a mask. So we just need to classify if the input frames contain live people. For this stage, we can use a 3D convolutional network in our model. Why 3D you asked? Cause a frame does not contain some very useful features like eye movement, winkings etc. RNN also would work great but that part on you if you want to try out. (Please don't forget to send it to me ðŸ˜„) 

A simple implementation of the liveness detection is as follows: 


```python

# X contains [n, 24, 100, 100, 1] arrays of image. Which is 24 frame long video in gray scale
# Y containt [n, 2] labels for per video input in one hot 

# Model
model = Sequential()
model.add(Conv3D(32, kernel_size=(3, 3, 3),
                 activation='relu',
                 input_shape=(24,100,100,1)))
model.add(Conv3D(64, (3, 3, 3), activation='relu'))
model.add(MaxPooling3D(pool_size=(2, 2, 2)))
model.add(Conv3D(64, (3, 3, 3), activation='relu'))
model.add(MaxPooling3D(pool_size=(2, 2, 2)))
model.add(Conv3D(64, (3, 3, 3), activation='relu'))
model.add(MaxPooling3D(pool_size=(2, 2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))


```

Next step is basic supervised training. 

When you are using in the system you also need to input 24 frames long array. So first you need to skip 23 frames and save it in the memory then you can start predicting the liveness of the input. 


Now, these two big problems should be solved. Please note that these two methods are not the best but they get the job done, but just keep in mind that I wouldn't use this project on my houses entrance ðŸ˜…  Just use it in the meeting room or something like an office(if you don't keep something valuable. ðŸ˜„ 


In conclusion, You can find all this code in [this repository](https://github.com/AhmetHamzaEmra/Intelegent_Lock).

Have fun! 

### TLDR:

- AI & ML good
- Landmark model for face recognition is crucial 
- Liveness is also crucial
- Not the safest lock  (at least for now)